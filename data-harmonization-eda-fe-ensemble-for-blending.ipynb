{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12699678,"sourceType":"datasetVersion","datasetId":8025996}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**ðŸŽ§ When Tempo Becomes Predictable  Light EDA, Deep Feature Engineering & Blending**\n\nThis project aims to predict the tempo (BPM) of songs from extracted features. Instead of a deep, exhaustive EDA, I focused primarily on feature engineering and model prediction to maximize predictive performance.\n\nThe core idea: transform raw signals into informative features, train several regression models (Ridge, ElasticNet, LightGBM, CatBoost), and combine their strengths with a weighted blending to minimize RMSE. The notebook is structured for practical effectiveness: careful feature creation, robust validation (OOF / K-Fold), hyperparameter optimization, and a final blend of predictions.\n\n> What youâ€™ll find in this notebook:\n\nðŸ”Ž A brief EDA to check data quality and spot anomalies.\n\nðŸ› ï¸ Extensive feature engineering (transformations, interactions, aggregations, encodings) â€” the main focus here.\n\nâš™ï¸ Training and tuning of multiple regression models.\n\nðŸ”— A final blending step to improve robustness and reduce prediction error.\n\nðŸ“ˆ Final evaluation and suggestions for further improvement (using raw audio, adding public datasets, or engineering new features).\n\n> Goal: minimize RMSE and produce stable, generalizable predictions. If you find this notebook useful, please leave an upvote â­ â€” it helps visibility and motivation to share reproducible solutions.","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries & Load Data\n","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:01.657349Z","iopub.execute_input":"2025-09-16T14:02:01.657716Z","iopub.status.idle":"2025-09-16T14:02:01.671649Z","shell.execute_reply.started":"2025-09-16T14:02:01.657688Z","shell.execute_reply":"2025-09-16T14:02:01.670059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"                       \nimport sys                         \nimport gc                          \nimport random                      \nimport warnings                    \nfrom datetime import datetime      \n\nSEED = 42\nrandom.seed(SEED)\n\n\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np                 \nimport pandas as pd                \n\n\nimport matplotlib.pyplot as plt    \nimport seaborn as sns              \nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nfrom scipy import stats           \nfrom collections import Counter    \nfrom scipy.stats import fisher_exact\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import skew\n\n\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport joblib   \n\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:02.940032Z","iopub.execute_input":"2025-09-16T14:02:02.940514Z","iopub.status.idle":"2025-09-16T14:02:06.457954Z","shell.execute_reply.started":"2025-09-16T14:02:02.940471Z","shell.execute_reply":"2025-09-16T14:02:06.457033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e9/train.csv')\ntest= pd.read_csv('/kaggle/input/playground-series-s5e9/test.csv')\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:08.420719Z","iopub.execute_input":"2025-09-16T14:02:08.422238Z","iopub.status.idle":"2025-09-16T14:02:10.632513Z","shell.execute_reply.started":"2025-09-16T14:02:08.422176Z","shell.execute_reply":"2025-09-16T14:02:10.631646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original = pd.read_csv('/kaggle/input/bpm-prediction-challenge/Train.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:15.361332Z","iopub.execute_input":"2025-09-16T14:02:15.361695Z","iopub.status.idle":"2025-09-16T14:02:15.391220Z","shell.execute_reply.started":"2025-09-16T14:02:15.361668Z","shell.execute_reply":"2025-09-16T14:02:15.389969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:19.665150Z","iopub.execute_input":"2025-09-16T14:02:19.665533Z","iopub.status.idle":"2025-09-16T14:02:19.681191Z","shell.execute_reply.started":"2025-09-16T14:02:19.665497Z","shell.execute_reply":"2025-09-16T14:02:19.680068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.concat([train, original], ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:24.767504Z","iopub.execute_input":"2025-09-16T14:02:24.767876Z","iopub.status.idle":"2025-09-16T14:02:24.819526Z","shell.execute_reply.started":"2025-09-16T14:02:24.767847Z","shell.execute_reply":"2025-09-16T14:02:24.818494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:26.656871Z","iopub.execute_input":"2025-09-16T14:02:26.657242Z","iopub.status.idle":"2025-09-16T14:02:26.663751Z","shell.execute_reply.started":"2025-09-16T14:02:26.657216Z","shell.execute_reply":"2025-09-16T14:02:26.662590Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\n# ===  general overview ===\nprint(\"Shapes -> train:\", train.shape, \" test:\", test.shape)\nprint(\"\\nColumns:\", train.columns.tolist())\nprint(\"\\n-- head --\")\nprint(train.head())\nprint(\"\\n-- info --\")\nprint(train.info())\nprint(\"\\n-- describe (numeric) --\")\nprint(train.describe().T)\n\n# ===  missing values & duplicate ids ===\nprint(\"\\nMissing values (train):\")\nprint(train.isnull().sum().loc[lambda x: x>0])\nprint(\"\\nMissing values (test):\")\nprint(test.isnull().sum().loc[lambda x: x>0])\n\nprint(\"\\nDuplicate ids (train):\", train['id'].duplicated().sum())\nprint(\"Duplicate ids (test):\", test['id'].duplicated().sum())\n\n# ===  target: distribution  ===\nprint(\"\\nBeatsPerMinute summary:\")\nprint(train['BeatsPerMinute'].describe())\nprint(\"Skew:\", train['BeatsPerMinute'].skew(), \"Kurtosis:\", train['BeatsPerMinute'].kurtosis())\n\nplt.figure(figsize=(8,4))\nsns.histplot(train['BeatsPerMinute'], kde=True, bins=60)\nplt.title(\"Distribution of BeatsPerMinute\")\nplt.xlabel(\"BeatsPerMinute\")\nplt.show()\n\nplt.figure(figsize=(6,2))\nplt.boxplot(train['BeatsPerMinute'], vert=False)\nplt.title(\"Boxplot BeatsPerMinute\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T23:55:15.369567Z","iopub.execute_input":"2025-09-15T23:55:15.369797Z","iopub.status.idle":"2025-09-15T23:55:19.086033Z","shell.execute_reply.started":"2025-09-15T23:55:15.369780Z","shell.execute_reply":"2025-09-15T23:55:19.085269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_cols = [col for col in train.columns if col not in [\"id\", \"BeatsPerMinute\"]]\n\nprint(\"\\nPlotting distributions of explanatory variables...\")\nfor f in feature_cols:\n    plt.figure(figsize=(6,3))\n    sns.histplot(train[f], kde=True, bins=50, color=\"steelblue\")\n    plt.title(f\"Distribution of {f}\")\n    plt.xlabel(f)\n    plt.ylabel(\"Count\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T23:55:19.086945Z","iopub.execute_input":"2025-09-15T23:55:19.087251Z","iopub.status.idle":"2025-09-15T23:55:42.883776Z","shell.execute_reply.started":"2025-09-15T23:55:19.087224Z","shell.execute_reply":"2025-09-15T23:55:42.882863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfeature_cols = [c for c in train.columns if c not in ['id', 'BeatsPerMinute']]\nprint(\"\\nFeatures considered:\", feature_cols)\n\n\n\ncorr = train[feature_cols + ['BeatsPerMinute']].corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0)\nplt.title(\"Correlation matrix (features + target)\")\nplt.show()\n\n# top features correlated with target\nprint(\"\\nAbsolute correlation with target (sorted):\")\nprint(corr['BeatsPerMinute'].abs().sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T23:55:42.884469Z","iopub.execute_input":"2025-09-15T23:55:42.884712Z","iopub.status.idle":"2025-09-15T23:55:43.648737Z","shell.execute_reply.started":"2025-09-15T23:55:42.884693Z","shell.execute_reply":"2025-09-15T23:55:43.647785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===  boxplots of explanatory variables ===\nprint(\"\\nPlotting boxplots of explanatory variables...\")\nfor f in feature_cols:\n    plt.figure(figsize=(6,2))\n    sns.boxplot(x=train[f], color=\"lightcoral\")\n    plt.title(f\"Boxplot of {f}\")\n    plt.xlabel(f)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T23:55:43.649689Z","iopub.execute_input":"2025-09-15T23:55:43.649962Z","iopub.status.idle":"2025-09-15T23:55:45.329439Z","shell.execute_reply.started":"2025-09-15T23:55:43.649942Z","shell.execute_reply":"2025-09-15T23:55:45.328379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âš™ï¸ Feature Engineering","metadata":{"execution":{"iopub.status.busy":"2025-09-06T20:32:43.783169Z","iopub.execute_input":"2025-09-06T20:32:43.783911Z","iopub.status.idle":"2025-09-06T20:32:43.789273Z","shell.execute_reply.started":"2025-09-06T20:32:43.783875Z","shell.execute_reply":"2025-09-06T20:32:43.788368Z"}}},{"cell_type":"code","source":"def generate_features(df_input):\n    numeric_columns = df_input.select_dtypes(include=[np.number]).columns\n    df_input[numeric_columns] = df_input[numeric_columns].fillna(df_input[numeric_columns].median())\n\n    df_input['TrackDurationSec'] = df_input['TrackDurationMs'] / 1000.0\n    df_input['Rhythm_x_Energy'] = df_input['RhythmScore'] * df_input['Energy']\n    df_input['Energy_x_AudioLoudness'] = df_input['Energy'] * df_input['AudioLoudness']\n    df_input['Rhythm_Audio_Mult'] = df_input['RhythmScore'] * df_input['AudioLoudness']\n    df_input['Vocal_Acoustic_Ratio'] = df_input['VocalContent'] / (df_input['AcousticQuality'] + 1e-6)\n    df_input['Energy_Mood_Mult'] = df_input['Energy'] * df_input['MoodScore']\n    df_input['Instrumental_Live_Mult'] = df_input['InstrumentalScore'] * df_input['LivePerformanceLikelihood']\n\n    poly_transformer = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n    poly_vals = poly_transformer.fit_transform(df_input[['RhythmScore', 'AudioLoudness', 'Energy']])\n    poly_columns = [f'poly_feat_{i}' for i in range(poly_vals.shape[1])]\n    df_input[poly_columns] = poly_vals\n\n    for col in ['TrackDurationMs', 'AudioLoudness', 'VocalContent']:\n        if col in df_input.columns and skew(df_input[col].dropna()) > 0.5:\n            if df_input[col].min() < 0:\n                shift_val = abs(df_input[col].min()) + 1\n                df_input[f'log_{col}'] = np.log1p(df_input[col] + shift_val)\n            else:\n                df_input[f'log_{col}'] = np.log1p(df_input[col].clip(lower=0))\n\n    df_input['Duration_Bin'] = pd.qcut(df_input['TrackDurationMs'], q=10, labels=False, duplicates='drop')\n    df_input['Energy_Bin'] = pd.qcut(df_input['Energy'], q=5, labels=False, duplicates='drop')\n    \n    return df_input\n\ntrain = generate_features(train)\ntest = generate_features(test)\n\nfeature_cols = [col for col in train.columns if col not in ['id', 'BeatsPerMinute'] and train[col].nunique() > 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:35.726830Z","iopub.execute_input":"2025-09-16T14:02:35.727669Z","iopub.status.idle":"2025-09-16T14:02:37.201163Z","shell.execute_reply.started":"2025-09-16T14:02:35.727637Z","shell.execute_reply":"2025-09-16T14:02:37.200209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ§¹ Data Preprocessing","metadata":{}},{"cell_type":"code","source":"y = train[\"BeatsPerMinute\"]\nX = train.drop(columns=[\"BeatsPerMinute\", \"id\"])  \n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)                     \nX_test_scaled = scaler.transform(test.drop(columns=[\"id\"])) \n\n\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=test.drop(columns=[\"id\"]).columns, index=test.index)\n\n\nprint(\"X_train shape:\", X_scaled.shape)\nprint(\"X_test shape:\", X_test_scaled.shape)\nprint(\"y shape:\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:41.640275Z","iopub.execute_input":"2025-09-16T14:02:41.640644Z","iopub.status.idle":"2025-09-16T14:02:42.391268Z","shell.execute_reply.started":"2025-09-16T14:02:41.640617Z","shell.execute_reply":"2025-09-16T14:02:42.389766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ¤– Modeling & Evaluation","metadata":{}},{"cell_type":"markdown","source":"****In this notebook, we directly used the best hyperparameters. However, earlier versions show the use of Optuna to select them. This approach was simplified to reduce execution time.****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy.optimize import minimize, differential_evolution\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nfrom scipy.stats import rankdata\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass AdvancedBlending:\n    def __init__(self, n_splits=5, random_state=42):\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.optimal_weights = None\n        self.meta_model = None\n        self.rank_weights = None\n        \n    def create_stratified_folds(self, y, n_bins=10):\n       \n        y_binned = pd.qcut(y, q=n_bins, labels=False, duplicates='drop')\n        return StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n    \n    def ensemble_predictions(self, models, X_train, y_train, X_test, use_stratified=True):\n        \n        n_models = len(models)\n        n_train = X_train.shape[0]\n        n_test = X_test.shape[0]\n        \n      \n        oof_preds = np.zeros((n_train, n_models))\n        test_preds = np.zeros((n_test, n_models))\n        feature_importance = {}\n        \n       \n        if use_stratified:\n            kf = self.create_stratified_folds(y_train)\n            folds = list(kf.split(X_train, pd.qcut(y_train, q=10, labels=False, duplicates='drop')))\n        else:\n            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n            folds = list(kf.split(X_train, y_train))\n        \n        fold_scores = []\n        \n        for fold, (train_idx, val_idx) in enumerate(folds):\n            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n            \n            fold_model_preds = []\n            \n            \n            for model_idx, (model_name, model_func, params) in enumerate(models):\n                try:\n                    if model_name == 'lgb':\n                        model = lgb.LGBMRegressor(**params)\n                        model.fit(\n                            X_fold_train, y_fold_train,\n                            eval_set=[(X_fold_val, y_fold_val)],\n                            callbacks=[lgb.early_stopping(150, verbose=False)]\n                        )\n                        if fold == 0:\n                            feature_importance[model_name] = model.feature_importances_\n                            \n                    elif model_name == 'cat':\n                        model = CatBoostRegressor(**params)\n                        model.fit(\n                            X_fold_train, y_fold_train,\n                            eval_set=(X_fold_val, y_fold_val),\n                            early_stopping_rounds=150,\n                            verbose=0\n                        )\n                        if fold == 0:\n                            feature_importance[model_name] = model.feature_importances_\n                            \n                    elif model_name == 'xgb':\n                        model = xgb.XGBRegressor(**params)\n                        model.fit(\n                            X_fold_train, y_fold_train,\n                            eval_set=[(X_fold_val, y_fold_val)],\n                            early_stopping_rounds=150,\n                            verbose=False\n                        )\n                        if fold == 0:\n                            feature_importance[model_name] = model.feature_importances_\n                    \n                   \n                    val_pred = model.predict(X_fold_val)\n                    oof_preds[val_idx, model_idx] = val_pred\n                    test_preds[:, model_idx] += model.predict(X_test) / self.n_splits\n                    fold_model_preds.append(val_pred)\n                    \n                except Exception as e:\n                    print(f\"Erreur avec le modÃ¨le {model_name} au fold {fold}: {str(e)}\")\n                    oof_preds[val_idx, model_idx] = np.mean(y_fold_train)\n                    test_preds[:, model_idx] += np.mean(y_fold_train) / self.n_splits\n                    fold_model_preds.append(np.full(len(val_idx), np.mean(y_fold_train)))\n            \n           \n            if len(fold_model_preds) > 0:\n                fold_blend = np.mean(fold_model_preds, axis=0)\n                fold_rmse = np.sqrt(mean_squared_error(y_fold_val, fold_blend))\n                fold_scores.append(fold_rmse)\n                print(f\"Fold {fold+1} RMSE (equal blend): {fold_rmse:.4f}\")\n        \n        print(f\"Moyenne CV RMSE (equal blend): {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n        self.feature_importance = feature_importance\n        return oof_preds, test_preds\n    \n    def optimize_weights_advanced(self, oof_preds, y_true):\n       \n        n_models = oof_preds.shape[1]\n        \n        \n        def objective_genetic(weights):\n            weights = np.abs(weights)\n            weights = weights / np.sum(weights)  # Normalisation\n            blended = np.dot(oof_preds, weights)\n            return np.sqrt(mean_squared_error(y_true, blended))\n        \n        bounds_genetic = [(0.001, 0.998)] * n_models\n        result_genetic = differential_evolution(\n            objective_genetic, bounds_genetic, \n            seed=self.random_state, maxiter=300, popsize=20\n        )\n        weights_genetic = np.abs(result_genetic.x)\n        weights_genetic = weights_genetic / np.sum(weights_genetic)\n        \n        \n        def objective_nlp(weights):\n            blended = np.dot(oof_preds, weights)\n            return np.sqrt(mean_squared_error(y_true, blended))\n        \n        constraints = [\n            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n            {'type': 'ineq', 'fun': lambda w: w}  # w >= 0\n        ]\n        bounds_nlp = [(0.001, 0.999)] * n_models\n        init_guess = np.ones(n_models) / n_models\n        \n        result_nlp = minimize(\n            objective_nlp, init_guess, \n            method='SLSQP', bounds=bounds_nlp, constraints=constraints\n        )\n        weights_nlp = result_nlp.x if result_nlp.success else weights_genetic\n        \n      \n        best_rmse_grid = float('inf')\n        best_weights_grid = None\n        \n       \n        step = 0.05\n        weight_ranges = []\n        for i in range(n_models):\n            weight_ranges.append(np.arange(0.0, 1.0 + step, step))\n        \n        \n        np.random.seed(self.random_state)\n        for _ in range(1000):  \n            weights = np.random.dirichlet(np.ones(n_models), size=1)[0]  \n            blended = np.dot(oof_preds, weights)\n            rmse = np.sqrt(mean_squared_error(y_true, blended))\n            if rmse < best_rmse_grid:\n                best_rmse_grid = rmse\n                best_weights_grid = weights.copy()\n        \n       \n        rmse_genetic = objective_genetic(weights_genetic)\n        rmse_nlp = objective_nlp(weights_nlp)\n        \n        results = [\n            ('Genetic Algorithm', weights_genetic, rmse_genetic),\n            ('NLP Optimization', weights_nlp, rmse_nlp),\n            ('Grid Search', best_weights_grid, best_rmse_grid)\n        ]\n        \n       \n        best_method, best_weights, best_rmse = min(results, key=lambda x: x[2])\n        \n        print(f\"\\nRÃ©sultats d'optimisation des poids:\")\n        for method, weights, rmse in results:\n            print(f\"{method}: RMSE = {rmse:.6f}, Poids = {weights}\")\n        print(f\"\\nMeilleure mÃ©thode: {best_method} (RMSE: {best_rmse:.6f})\")\n        \n        self.optimal_weights = best_weights\n        return best_weights\n    \n    def train_meta_model(self, oof_preds, y_true):\n       \n        meta_models = {\n            'ridge': Ridge(alpha=10.0, random_state=self.random_state),\n            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state),\n            'rf': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=self.random_state),\n            'mlp': MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=500, random_state=self.random_state)\n        }\n        \n        best_meta_model = None\n        best_meta_rmse = float('inf')\n        meta_results = {}\n        \n       \n        kf = KFold(n_splits=3, shuffle=True, random_state=self.random_state)\n        \n        for name, model in meta_models.items():\n            cv_scores = []\n            for train_idx, val_idx in kf.split(oof_preds):\n                X_meta_train, X_meta_val = oof_preds[train_idx], oof_preds[val_idx]\n                y_meta_train, y_meta_val = y_true[train_idx], y_true[val_idx]\n                \n                try:\n                    model.fit(X_meta_train, y_meta_train)\n                    pred_meta = model.predict(X_meta_val)\n                    rmse = np.sqrt(mean_squared_error(y_meta_val, pred_meta))\n                    cv_scores.append(rmse)\n                except:\n                    cv_scores.append(float('inf'))\n            \n            avg_rmse = np.mean(cv_scores)\n            meta_results[name] = avg_rmse\n            \n            if avg_rmse < best_meta_rmse:\n                best_meta_rmse = avg_rmse\n                best_meta_model = model\n        \n       \n        if best_meta_model is not None:\n            best_meta_model.fit(oof_preds, y_true)\n            self.meta_model = best_meta_model\n        \n        print(f\"\\nRÃ©sultats des meta-modÃ¨les:\")\n        for name, rmse in meta_results.items():\n            print(f\"{name}: {rmse:.6f}\")\n        print(f\"Meilleur meta-modÃ¨le: RMSE = {best_meta_rmse:.6f}\")\n        \n        return best_meta_model\n    \n    def rank_averaging(self, oof_preds, test_preds, y_true):\n        \n        n_models = oof_preds.shape[1]\n        \n        \n        oof_ranks = np.zeros_like(oof_preds)\n        test_ranks = np.zeros_like(test_preds)\n        \n        for i in range(n_models):\n            oof_ranks[:, i] = rankdata(oof_preds[:, i])\n            test_ranks[:, i] = rankdata(test_preds[:, i])\n        \n       \n        def rank_objective(weights):\n            weights = np.abs(weights)\n            weights = weights / np.sum(weights)\n            blended_ranks = np.dot(oof_ranks, weights)\n            return np.sqrt(mean_squared_error(y_true, blended_ranks))\n        \n        bounds = [(0.001, 0.999)] * n_models\n        result = differential_evolution(\n            rank_objective, bounds, seed=self.random_state, maxiter=200\n        )\n        \n        rank_weights = np.abs(result.x)\n        rank_weights = rank_weights / np.sum(rank_weights)\n        self.rank_weights = rank_weights\n        \n        return rank_weights\n    \n    def final_blend(self, oof_preds, test_preds, y_true):\n        \n        print(\"=== BLENDING AVANCÃ‰ ===\")\n        \n       \n        optimal_weights = self.optimize_weights_advanced(oof_preds, y_true)\n        blend_optimal = np.dot(oof_preds, optimal_weights)\n        rmse_optimal = np.sqrt(mean_squared_error(y_true, blend_optimal))\n        \n        \n        meta_model = self.train_meta_model(oof_preds, y_true)\n        blend_meta = meta_model.predict(oof_preds) if meta_model else blend_optimal\n        rmse_meta = np.sqrt(mean_squared_error(y_true, blend_meta)) if meta_model else rmse_optimal\n        \n        \n        rank_weights = self.rank_averaging(oof_preds, test_preds, y_true)\n        oof_ranks = np.zeros_like(oof_preds)\n        for i in range(oof_preds.shape[1]):\n            oof_ranks[:, i] = rankdata(oof_preds[:, i])\n        blend_rank = np.dot(oof_ranks, rank_weights)\n        rmse_rank = np.sqrt(mean_squared_error(y_true, blend_rank))\n        \n       \n        methods = [\n            ('Optimal Weights', blend_optimal, rmse_optimal, 'weights'),\n            ('Meta Model', blend_meta, rmse_meta, 'meta'),\n            ('Rank Averaging', blend_rank, rmse_rank, 'rank')\n        ]\n        \n        best_method, best_blend, best_rmse, best_type = min(methods, key=lambda x: x[2])\n        \n        print(f\"\\n=== RÃ‰SULTATS FINAUX ===\")\n        for method, _, rmse, _ in methods:\n            print(f\"{method}: RMSE = {rmse:.6f}\")\n        print(f\"\\nMeilleure mÃ©thode: {best_method} (RMSE: {best_rmse:.6f})\")\n        \n       \n        if best_type == 'weights':\n            final_test_preds = np.dot(test_preds, optimal_weights)\n        elif best_type == 'meta' and meta_model:\n            final_test_preds = meta_model.predict(test_preds)\n        else:  # rank\n            test_ranks = np.zeros_like(test_preds)\n            for i in range(test_preds.shape[1]):\n                test_ranks[:, i] = rankdata(test_preds[:, i])\n            final_test_preds = np.dot(test_ranks, rank_weights)\n        \n        return final_test_preds, best_rmse, best_method\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:02:51.071559Z","iopub.execute_input":"2025-09-16T14:02:51.071928Z","iopub.status.idle":"2025-09-16T14:02:51.114447Z","shell.execute_reply.started":"2025-09-16T14:02:51.071901Z","shell.execute_reply":"2025-09-16T14:02:51.113180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_params = {\n    \"colsample_bytree\": 0.6977764240714932,\n    \"learning_rate\": 0.010200660341891958,\n    \"max_depth\": 3,\n    \"min_child_samples\": 7,\n    \"n_estimators\": 711,\n    \"num_leaves\": 190,\n    \"reg_alpha\": 1.1411660265926687e-05,\n    \"reg_lambda\": 8.328266237197566,\n    \"subsample\": 0.5254691782296235,\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"boosting_type\": \"gbdt\",\n    \"verbosity\": -1,\n    \"random_state\": 42\n}\n\ncat_params = {\n    \"iterations\": 1856,\n    \"learning_rate\": 0.0034955405986767337,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 0.013723978895316472,\n    \"bagging_temperature\": 0.13216576435438537,\n    \"border_count\": 99,\n    \"loss_function\": \"RMSE\",\n    \"eval_metric\": \"RMSE\",\n    \"random_seed\": 42,\n    \"verbose\": 0\n}\n\nxgb_params = {\n    'n_estimators': 264,\n    'learning_rate': 0.016147510440075844,\n    'max_depth': 3,\n    'subsample': 0.6373874482675941,\n    'colsample_bytree': 0.5937026385399601,\n    'reg_alpha': 0.00034470953918926047,\n    'reg_lambda': 1.6911581294744957e-07,\n    'min_child_weight': 4,\n    'objective': 'reg:squarederror',\n    'random_state': 42,\n    'verbosity': 0\n}\n\n\nX_scaled = X_scaled.values if hasattr(X_scaled, \"values\") else X_scaled\nX_test_scaled = X_test_scaled.values if hasattr(X_test_scaled, \"values\") else X_test_scaled\ny = y.values if hasattr(y, \"values\") else y\n\n\nmodels = [\n    ('lgb', 'lgb', lgb_params),\n    ('cat', 'cat', cat_params),\n    ('xgb', 'xgb', xgb_params)\n]\n\n\nadvanced_blender = AdvancedBlending(n_splits=5, random_state=42)\n\n\nprint(\"GÃ©nÃ©ration des prÃ©dictions avec validation croisÃ©e stratifiÃ©e...\")\noof_preds, test_preds = advanced_blender.ensemble_predictions(\n    models, X_scaled, y, X_test_scaled, use_stratified=True\n)\n\n\nfinal_predictions, final_rmse, best_method = advanced_blender.final_blend(\n    oof_preds, test_preds, y\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:03:10.207642Z","iopub.execute_input":"2025-09-16T14:03:10.208074Z","iopub.status.idle":"2025-09-16T14:19:44.821789Z","shell.execute_reply.started":"2025-09-16T14:03:10.208042Z","shell.execute_reply":"2025-09-16T14:19:44.820766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def intelligent_clipping(predictions, train_target, percentile_range=(1, 99)):\n    \"\"\"Clipping intelligent basÃ© sur les percentiles des donnÃ©es d'entraÃ®nement\"\"\"\n    lower_bound = np.percentile(train_target, percentile_range[0])\n    upper_bound = np.percentile(train_target, percentile_range[1])\n    \n    \n    range_extension = (upper_bound - lower_bound) * 0.05\n    lower_bound = max(lower_bound - range_extension, 40)  \n    upper_bound = min(upper_bound + range_extension, 200)  \n    \n    return np.clip(predictions, lower_bound, upper_bound)\n\n\nfinal_predictions = intelligent_clipping(final_predictions, y)\n\nprint(f\"\\n=== RÃ‰SULTAT FINAL ===\")\nprint(f\"MÃ©thode choisie: {best_method}\")\nprint(f\"RMSE final: {final_rmse:.6f}\")\nprint(f\"Plage des prÃ©dictions finales: [{final_predictions.min():.2f}, {final_predictions.max():.2f}]\")\n\n\nif advanced_blender.optimal_weights is not None:\n    print(f\"\\nPoids optimaux:\")\n    model_names = ['LightGBM', 'CatBoost', 'XGBoost']\n    for i, (name, weight) in enumerate(zip(model_names, advanced_blender.optimal_weights)):\n        print(f\"  {name}: {weight:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T14:19:44.823125Z","iopub.execute_input":"2025-09-16T14:19:44.823431Z","iopub.status.idle":"2025-09-16T14:19:44.852661Z","shell.execute_reply.started":"2025-09-16T14:19:44.823401Z","shell.execute_reply":"2025-09-16T14:19:44.851624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“¤ Submission File Creation","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/playground-series-s5e9/sample_submission.csv')\nsubmission['BeatsPerMinute'] = final_predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}